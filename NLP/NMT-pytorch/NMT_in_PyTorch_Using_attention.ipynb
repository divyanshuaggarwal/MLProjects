{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT in PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gau9xEXMGY8s",
        "colab_type": "text"
      },
      "source": [
        "# Neural Machine Translation with Attention Using PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z579-ISl9Zj6",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT20LFmb3jSW",
        "colab_type": "code",
        "outputId": "83b8a283-d8e4-4e6a-de5a-d3ffb3b3276d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "import time\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7.0+cu110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAuXJjo9NuT8",
        "colab_type": "text"
      },
      "source": [
        "## Downloading Data\n",
        "obtain it from [here](http://www.manythings.org/anki/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Ox1YURzVhF",
        "colab_type": "code",
        "outputId": "e6393ac2-a310-4449-af21-5bd731c17239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!wget http://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2020-10-28 13:53:11--  http://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 172.67.173.198, 104.24.109.196, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4812467 (4.6M) [application/zip]\n",
            "Saving to: 'spa-eng.zip'\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  1%  118K 39s\n",
            "    50K .......... .......... .......... .......... ..........  2%  260K 28s\n",
            "   100K .......... .......... .......... .......... ..........  3% 1.40M 20s\n",
            "   150K .......... .......... .......... .......... ..........  4% 1.26M 16s\n",
            "   200K .......... .......... .......... .......... ..........  5%  308K 15s\n",
            "   250K .......... .......... .......... .......... ..........  6% 2.72M 13s\n",
            "   300K .......... .......... .......... .......... ..........  7% 2.74M 11s\n",
            "   350K .......... .......... .......... .......... ..........  8%  956K 10s\n",
            "   400K .......... .......... .......... .......... ..........  9%  539K 10s\n",
            "   450K .......... .......... .......... .......... .......... 10% 3.15M 9s\n",
            "   500K .......... .......... .......... .......... .......... 11% 4.23M 8s\n",
            "   550K .......... .......... .......... .......... .......... 12% 2.23M 7s\n",
            "   600K .......... .......... .......... .......... .......... 13% 4.16M 7s\n",
            "   650K .......... .......... .......... .......... .......... 14% 2.19M 6s\n",
            "   700K .......... .......... .......... .......... .......... 15% 1.72M 6s\n",
            "   750K .......... .......... .......... .......... .......... 17% 2.50M 6s\n",
            "   800K .......... .......... .......... .......... .......... 18% 3.53M 5s\n",
            "   850K .......... .......... .......... .......... .......... 19%  749K 5s\n",
            "   900K .......... .......... .......... .......... .......... 20% 4.16M 5s\n",
            "   950K .......... .......... .......... .......... .......... 21% 3.57M 5s\n",
            "  1000K .......... .......... .......... .......... .......... 22% 4.85M 4s\n",
            "  1050K .......... .......... .......... .......... .......... 23% 4.87M 4s\n",
            "  1100K .......... .......... .......... .......... .......... 24% 6.41M 4s\n",
            "  1150K .......... .......... .......... .......... .......... 25% 4.15M 4s\n",
            "  1200K .......... .......... .......... .......... .......... 26% 4.28M 4s\n",
            "  1250K .......... .......... .......... .......... .......... 27% 4.65M 3s\n",
            "  1300K .......... .......... .......... .......... .......... 28% 3.61M 3s\n",
            "  1350K .......... .......... .......... .......... .......... 29% 3.20M 3s\n",
            "  1400K .......... .......... .......... .......... .......... 30% 3.01M 3s\n",
            "  1450K .......... .......... .......... .......... .......... 31% 4.22M 3s\n",
            "  1500K .......... .......... .......... .......... .......... 32% 5.00M 3s\n",
            "  1550K .......... .......... .......... .......... .......... 34% 2.77M 3s\n",
            "  1600K .......... .......... .......... .......... .......... 35% 3.61M 3s\n",
            "  1650K .......... .......... .......... .......... .......... 36% 5.86M 3s\n",
            "  1700K .......... .......... .......... .......... .......... 37% 5.88M 2s\n",
            "  1750K .......... .......... .......... .......... .......... 38% 5.26M 2s\n",
            "  1800K .......... .......... .......... .......... .......... 39% 4.36M 2s\n",
            "  1850K .......... .......... .......... .......... .......... 40% 3.08M 2s\n",
            "  1900K .......... .......... .......... .......... .......... 41% 3.61M 2s\n",
            "  1950K .......... .......... .......... .......... .......... 42% 4.14M 2s\n",
            "  2000K .......... .......... .......... .......... .......... 43% 3.80M 2s\n",
            "  2050K .......... .......... .......... .......... .......... 44% 2.52M 2s\n",
            "  2100K .......... .......... .......... .......... .......... 45% 6.25M 2s\n",
            "  2150K .......... .......... .......... .......... .......... 46% 4.84M 2s\n",
            "  2200K .......... .......... .......... .......... .......... 47% 68.2K 2s\n",
            "  2250K .......... .......... .......... .......... .......... 48%  573K 2s\n",
            "  2300K .......... .......... .......... .......... .......... 50% 53.3K 3s\n",
            "  2350K .......... .......... .......... .......... .......... 51%  204K 3s\n",
            "  2400K .......... .......... .......... .......... .......... 52%  560K 3s\n",
            "  2450K .......... .......... .......... .......... .......... 53%  647K 3s\n",
            "  2500K .......... .......... .......... .......... .......... 54%  711K 3s\n",
            "  2550K .......... .......... .......... .......... .......... 55%  761K 3s\n",
            "  2600K .......... .......... .......... .......... .......... 56%  831K 3s\n",
            "  2650K .......... .......... .......... .......... .......... 57%  980K 3s\n",
            "  2700K .......... .......... .......... .......... .......... 58%  947K 3s\n",
            "  2750K .......... .......... .......... .......... .......... 59% 1.03M 3s\n",
            "  2800K .......... .......... .......... .......... .......... 60% 1.11M 3s\n",
            "  2850K .......... .......... .......... .......... .......... 61% 1.18M 3s\n",
            "  2900K .......... .......... .......... .......... .......... 62% 1.21M 2s\n",
            "  2950K .......... .......... .......... .......... .......... 63% 1.25M 2s\n",
            "  3000K .......... .......... .......... .......... .......... 64% 3.77M 2s\n",
            "  3050K .......... .......... .......... .......... .......... 65% 2.48M 2s\n",
            "  3100K .......... .......... .......... .......... .......... 67% 1.09M 2s\n",
            "  3150K .......... .......... .......... .......... .......... 68% 1.28M 2s\n",
            "  3200K .......... .......... .......... .......... .......... 69% 1.29M 2s\n",
            "  3250K .......... .......... .......... .......... .......... 70% 1.51M 2s\n",
            "  3300K .......... .......... .......... .......... .......... 71% 1.34M 2s\n",
            "  3350K .......... .......... .......... .......... .......... 72% 1.42M 2s\n",
            "  3400K .......... .......... .......... .......... .......... 73% 1.29M 2s\n",
            "  3450K .......... .......... .......... .......... .......... 74% 1.32M 2s\n",
            "  3500K .......... .......... .......... .......... .......... 75% 1.46M 1s\n",
            "  3550K .......... .......... .......... .......... .......... 76% 1.39M 1s\n",
            "  3600K .......... .......... .......... .......... .......... 77% 1.42M 1s\n",
            "  3650K .......... .......... .......... .......... .......... 78% 1.17M 1s\n",
            "  3700K .......... .......... .......... .......... .......... 79% 1.36M 1s\n",
            "  3750K .......... .......... .......... .......... .......... 80% 1.50M 1s\n",
            "  3800K .......... .......... .......... .......... .......... 81% 1.58M 1s\n",
            "  3850K .......... .......... .......... .......... .......... 82% 1.92M 1s\n",
            "  3900K .......... .......... .......... .......... .......... 84% 1.79M 1s\n",
            "  3950K .......... .......... .......... .......... .......... 85%  897K 1s\n",
            "  4000K .......... .......... .......... .......... .......... 86%  475K 1s\n",
            "  4050K .......... .......... .......... .......... .......... 87% 82.1M 1s\n",
            "  4100K .......... .......... .......... .......... .......... 88% 94.8M 1s\n",
            "  4150K .......... .......... .......... .......... .......... 89%  148M 1s\n",
            "  4200K .......... .......... .......... .......... .......... 90% 1.24M 1s\n",
            "  4250K .......... .......... .......... .......... .......... 91% 1.85M 0s\n",
            "  4300K .......... .......... .......... .......... .......... 92% 1.52M 0s\n",
            "  4350K .......... .......... .......... .......... .......... 93% 1.99M 0s\n",
            "  4400K .......... .......... .......... .......... .......... 94% 1.46M 0s\n",
            "  4450K .......... .......... .......... .......... .......... 95% 1.97M 0s\n",
            "  4500K .......... .......... .......... .......... .......... 96% 1.56M 0s\n",
            "  4550K .......... .......... .......... .......... .......... 97% 1.88M 0s\n",
            "  4600K .......... .......... .......... .......... .......... 98%  208K 0s\n",
            "  4650K .......... .......... .......... .......... ......... 100%  131K=5.9s\n",
            "\n",
            "2020-10-28 13:53:19 (798 KB/s) - 'spa-eng.zip' saved [4812467/4812467]\n",
            "\n",
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD8Qy0eC0ZtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('spa.txt', encoding='UTF-8').read().strip().split('\\n')  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mVlB0W14b4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = f"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JouLb6Eo4f28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample size (try with smaller sample size to reduce computation)\n",
        "num_examples = 30000 \n",
        "\n",
        "# creates lists containing each pair\n",
        "original_word_pairs = [[w for w in l.split('\\t')] for l in lines[:num_examples]]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as2-5vGn4jUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.DataFrame(original_word_pairs, columns=[\"eng\", \"es\",\"bleh\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D549RQlnRluV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop(columns = \"bleh\",inplace = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "913VSLih4lY3",
        "colab_type": "code",
        "outputId": "329b15c4-f8bb-4629-b3e6-8be939d39ac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   eng       es\n",
              "0  Go.      Ve.\n",
              "1  Go.    Vete.\n",
              "2  Go.    Vaya.\n",
              "3  Go.  Váyase.\n",
              "4  Hi.    Hola."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>es</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Go.</td>\n      <td>Ve.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Go.</td>\n      <td>Vete.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Go.</td>\n      <td>Vaya.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Go.</td>\n      <td>Váyase.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hi.</td>\n      <td>Hola.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCUSf31E4m6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    \"\"\"\n",
        "    Normalizes latin chars with accent to their canonical decomposition\n",
        "    \"\"\"\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN2pLaZkNqrv",
        "colab_type": "text"
      },
      "source": [
        "## Data Exploration\n",
        "Let's explore the dataset a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFLV4RCR4pXa",
        "colab_type": "code",
        "outputId": "f0f93d09-bf07-46ed-a8a7-8c4b4e06a4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# Now we do the preprocessing using pandas and lambdas\n",
        "data[\"eng\"] = data.eng.apply(lambda w: preprocess_sentence(w))\n",
        "data[\"es\"] = data.es.apply(lambda w: preprocess_sentence(w))\n",
        "data.sample(10)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        eng  \\\n",
              "263                <start> am i fat ? <end>   \n",
              "19398    <start> we can t stay here . <end>   \n",
              "16570    <start> did tom eat dinner ? <end>   \n",
              "964               <start> we ll see . <end>   \n",
              "20522   <start> everything is ready . <end>   \n",
              "27935  <start> you re a real friend . <end>   \n",
              "10773      <start> he is still here . <end>   \n",
              "11927      <start> now try to sleep . <end>   \n",
              "21878   <start> it makes me nervous . <end>   \n",
              "18893    <start> this is your doing . <end>   \n",
              "\n",
              "                                              es  \n",
              "263                <start> ¿ estoy gordo ? <end>  \n",
              "19398  <start> no podemos quedarnos aqui . <end>  \n",
              "16570                 <start> ¿ tom ceno ? <end>  \n",
              "964                      <start> veremos . <end>  \n",
              "20522        <start> todo esta preparado . <end>  \n",
              "27935    <start> eres un verdadero amigo . <end>  \n",
              "10773          <start> todavia esta aqui . <end>  \n",
              "11927      <start> ahora intenten dormir . <end>  \n",
              "21878           <start> me pone nervioso . <end>  \n",
              "18893          <start> esto es cosa tuya . <end>  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>es</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>263</th>\n      <td>&lt;start&gt; am i fat ? &lt;end&gt;</td>\n      <td>&lt;start&gt; ¿ estoy gordo ? &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>19398</th>\n      <td>&lt;start&gt; we can t stay here . &lt;end&gt;</td>\n      <td>&lt;start&gt; no podemos quedarnos aqui . &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>16570</th>\n      <td>&lt;start&gt; did tom eat dinner ? &lt;end&gt;</td>\n      <td>&lt;start&gt; ¿ tom ceno ? &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>&lt;start&gt; we ll see . &lt;end&gt;</td>\n      <td>&lt;start&gt; veremos . &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>20522</th>\n      <td>&lt;start&gt; everything is ready . &lt;end&gt;</td>\n      <td>&lt;start&gt; todo esta preparado . &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>27935</th>\n      <td>&lt;start&gt; you re a real friend . &lt;end&gt;</td>\n      <td>&lt;start&gt; eres un verdadero amigo . &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>10773</th>\n      <td>&lt;start&gt; he is still here . &lt;end&gt;</td>\n      <td>&lt;start&gt; todavia esta aqui . &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>11927</th>\n      <td>&lt;start&gt; now try to sleep . &lt;end&gt;</td>\n      <td>&lt;start&gt; ahora intenten dormir . &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>21878</th>\n      <td>&lt;start&gt; it makes me nervous . &lt;end&gt;</td>\n      <td>&lt;start&gt; me pone nervioso . &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>18893</th>\n      <td>&lt;start&gt; this is your doing . &lt;end&gt;</td>\n      <td>&lt;start&gt; esto es cosa tuya . &lt;end&gt;</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqM7ZncM8V9B",
        "colab_type": "text"
      },
      "source": [
        "## Building Vocabulary Index\n",
        "The class below is useful for creating the vocabular and index mappings which will be used to convert out inputs into indexed sequences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rXA7-N34sok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        \"\"\" lang are the list of phrases from each language\"\"\"\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        \n",
        "        self.create_index()\n",
        "        \n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            # update with individual tokens\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "            \n",
        "        # sort the vocab\n",
        "        self.vocab = sorted(self.vocab)\n",
        "\n",
        "        # add a padding token with index 0\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        \n",
        "        # word to index mapping\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
        "        \n",
        "        # index to word mapping\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fesymsn34v7z",
        "colab_type": "code",
        "outputId": "9a64f5d6-fef5-4974-cab2-475c567bb53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# index language using the class above\n",
        "inp_lang = LanguageIndex(data[\"es\"].values.tolist())\n",
        "targ_lang = LanguageIndex(data[\"eng\"].values.tolist())\n",
        "# Vectorize the input and target languages\n",
        "input_tensor = [[inp_lang.word2idx[s] for s in es.split(' ')]  for es in data[\"es\"].values.tolist()]\n",
        "target_tensor = [[targ_lang.word2idx[s] for s in eng.split(' ')]  for eng in data[\"eng\"].values.tolist()]\n",
        "input_tensor[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 9009, 3, 4],\n",
              " [5, 9128, 3, 4],\n",
              " [5, 9001, 3, 4],\n",
              " [5, 9008, 3, 4],\n",
              " [5, 4670, 3, 4],\n",
              " [5, 2267, 1, 4],\n",
              " [5, 2265, 1, 4],\n",
              " [5, 2264, 1, 4],\n",
              " [5, 2272, 1, 4],\n",
              " [5, 2272, 3, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPordlA-N4qR",
        "colab_type": "code",
        "outputId": "f44565d0-3bc4-4de9-bdcf-699275db945d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "target_tensor[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 1810, 3, 4],\n",
              " [5, 1810, 3, 4],\n",
              " [5, 1810, 3, 4],\n",
              " [5, 1810, 3, 4],\n",
              " [5, 2009, 3, 4],\n",
              " [5, 3574, 1, 4],\n",
              " [5, 3574, 1, 4],\n",
              " [5, 3574, 1, 4],\n",
              " [5, 3574, 1, 4],\n",
              " [5, 3574, 3, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cwX-0rt4zmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycYy5gq641Uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the max_length of input and output tensor\n",
        "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q05E5IwH42_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sequences(x, max_len):\n",
        "    padded = np.zeros((max_len), dtype=np.int64)\n",
        "    if len(x) > max_len: padded[:] = x[:max_len]\n",
        "    else: padded[:len(x)] = x\n",
        "    return padded"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66dJPqzV44jd",
        "colab_type": "code",
        "outputId": "362b9aaf-dad1-4de8-b44d-b01f41199f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# inplace padding\n",
        "input_tensor = [pad_sequences(x, max_length_inp) for x in input_tensor]\n",
        "target_tensor = [pad_sequences(x, max_length_tar) for x in target_tensor]\n",
        "len(target_tensor)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvatfCWS46T-",
        "colab_type": "code",
        "outputId": "ff7363bf-fa0e-481d-f034-dcab3377684e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 24000, 6000, 6000)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNFO3obpOsoB",
        "colab_type": "text"
      },
      "source": [
        "## Load data into DataLoader for Batching\n",
        "This is just preparing the dataset so that it can be efficiently fed into the model through batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QRQKwxf479Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDSxA4OM5Qlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# conver the data to tensors and pass to the Dataloader \n",
        "# to create an batch iterator\n",
        "\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        # TODO: convert this into torch code is possible\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2WukeVF8NVn",
        "colab_type": "text"
      },
      "source": [
        "## Parameters\n",
        "Let's define the hyperparameters and other things we need for training our NMT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Be7lOZ5R-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
        "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
        "\n",
        "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blYXo7pv5TOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
        "        \n",
        "    def forward(self, x, lens, device):\n",
        "        # x: batch_size, max_length \n",
        "        \n",
        "        # x: batch_size, max_length, embedding_dim\n",
        "        x = self.embedding(x) \n",
        "                \n",
        "        # x transformed = max_len X batch_size X embedding_dim\n",
        "        # x = x.permute(1,0,2)\n",
        "        x = pack_padded_sequence(x, lens) # unpad\n",
        "    \n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        \n",
        "        # output: max_length, batch_size, enc_units\n",
        "        # self.hidden: 1, batch_size, enc_units\n",
        "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "        \n",
        "        # pad the sequence to the max length in the batch\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        return output, self.hidden\n",
        "\n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrsQ7dTg5V__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### sort batch function to be able to use with pad_packed_sequence\n",
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X1h155CPQ1Y",
        "colab_type": "text"
      },
      "source": [
        "## Testing the Encoder\n",
        "Before proceeding with training, we should always try to test out model behavior such as the size of outputs just to make that things are going as expected. In PyTorch this can be done easily since everything comes in eager execution by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbSLACY45Xz-",
        "colab_type": "code",
        "outputId": "fed5ce73-cf1e-4358-f864-67199dcebac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Testing Encoder part\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "\n",
        "print(enc_output.size()) # max_length, batch_size, enc_units"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([9, 64, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4djvgil5bMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
        "                          self.dec_units,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.V = nn.Linear(self.enc_units, 1)\n",
        "    \n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        # enc_output original: (max_length, batch_size, enc_units)\n",
        "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
        "        enc_output = enc_output.permute(1,0,2)\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        \n",
        "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        # It doesn't matter which FC we pick for each of the inputs\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
        "          \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        # takes case of the right portion of the model above (illustrated in red)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        # ? Looks like attention vector in diagram of source\n",
        "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output: (batch_size, 1, hidden_size)\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output =  output.view(-1, output.size(2))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return torch.zeros((1, self.batch_sz, self.dec_units))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsG5We7Sk_UR",
        "colab_type": "text"
      },
      "source": [
        "## Testing the Decoder\n",
        "Similarily, try to test the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmipPRVx5fqO",
        "colab_type": "code",
        "outputId": "69bc5dad-af98-401e-8c01-80aa70867cd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "print(\"Input: \", x.shape)\n",
        "print(\"Output: \", y.shape)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "print(\"Encoder Output: \", enc_output.shape) # batch_size X max_length X enc_units\n",
        "print(\"Encoder Hidden: \", enc_hidden.shape) # batch_size X enc_units (corresponds to the last state)\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "#print(enc_hidden.squeeze(0).shape)\n",
        "\n",
        "dec_hidden = enc_hidden#.squeeze(0)\n",
        "dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "print(\"Decoder Input: \", dec_input.shape)\n",
        "print(\"--------\")\n",
        "\n",
        "for t in range(1, y.size(1)):\n",
        "    # enc_hidden: 1, batch_size, enc_units\n",
        "    # output: max_length, batch_size, enc_units\n",
        "    predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "    \n",
        "    print(\"Prediction: \", predictions.shape)\n",
        "    print(\"Decoder Hidden: \", dec_hidden.shape)\n",
        "    \n",
        "    #loss += loss_function(y[:, t].to(device), predictions.to(device))\n",
        "    \n",
        "    dec_input = y[:, t].unsqueeze(1)\n",
        "    print(dec_input.shape)\n",
        "    break"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  torch.Size([64, 16])\n",
            "Output:  torch.Size([64, 11])\n",
            "Encoder Output:  torch.Size([10, 64, 1024])\n",
            "Encoder Hidden:  torch.Size([1, 64, 1024])\n",
            "Decoder Input:  torch.Size([64, 1])\n",
            "--------\n",
            "Prediction:  torch.Size([64, 4815])\n",
            "Decoder Hidden:  torch.Size([1, 64, 1024])\n",
            "torch.Size([64, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QclyWIop5dRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
        "    #print(mask)\n",
        "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
        "    \n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjMMYJv85hVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## TODO: Combine the encoder and decoder into one class\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=0.001)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6_WoDZM7reU",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN8G-3YY8ADm",
        "colab_type": "code",
        "outputId": "f8732624-e99c-47b2-80d3-3671e4d8aca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "        enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        # use teacher forcing - feeding the target as the next input (via dec_input)\n",
        "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "        \n",
        "        # run code below for every timestep in the ys batch\n",
        "        for t in range(1, ys.size(1)):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "            loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "            #loss += loss_\n",
        "            dec_input = ys[:, t].unsqueeze(1)\n",
        "            \n",
        "        \n",
        "        batch_loss = (loss / int(ys.size(1)))\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.detach().item()))\n",
        "        \n",
        "        \n",
        "    ### TODO: Save checkpoint for model\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}