{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disease Extraction Hackathon - NER using DEEP LEARNING.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFZLYS1faH7o7hMWirAuPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyanshuaggarwal/MyProjects/blob/master/Disease_Extraction_Hackathon_NER_using_DEEP_LEARNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Dx_ZSe2j2y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "874b7c79-0a22-479d-8d38-b4e94b744786"
      },
      "source": [
        "!wget https://www.dropbox.com/s/ef5g11fdq7igi74/hackathon_disease_extraction.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-01 18:01:19--  https://www.dropbox.com/s/ef5g11fdq7igi74/hackathon_disease_extraction.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601f:1::a27d:901\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/ef5g11fdq7igi74/hackathon_disease_extraction.zip [following]\n",
            "--2020-04-01 18:01:19--  https://www.dropbox.com/s/raw/ef5g11fdq7igi74/hackathon_disease_extraction.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc83f0f5708a2a1a8cdda08ea965.dl.dropboxusercontent.com/cd/0/inline/A1C2Y-JU5kzazZr4DMlovFu0HnLqO4mNRJcbwgeq0sQicWsXjFk9EvxeEZ07guw9Y625YicDHI57cA3oMl0QCdbkd0i_GW2qSG0Fxr3Dm1TQIg/file# [following]\n",
            "--2020-04-01 18:01:19--  https://uc83f0f5708a2a1a8cdda08ea965.dl.dropboxusercontent.com/cd/0/inline/A1C2Y-JU5kzazZr4DMlovFu0HnLqO4mNRJcbwgeq0sQicWsXjFk9EvxeEZ07guw9Y625YicDHI57cA3oMl0QCdbkd0i_GW2qSG0Fxr3Dm1TQIg/file\n",
            "Resolving uc83f0f5708a2a1a8cdda08ea965.dl.dropboxusercontent.com (uc83f0f5708a2a1a8cdda08ea965.dl.dropboxusercontent.com)... 162.125.9.6, 2620:100:601f:6::a27d:906\n",
            "Connecting to uc83f0f5708a2a1a8cdda08ea965.dl.dropboxusercontent.com (uc83f0f5708a2a1a8cdda08ea965.dl.dropboxusercontent.com)|162.125.9.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/A1BEyAMnFjkhKExR56wPlhsmyQQ0Kn6LehO50zWN3Kwj2UIvyUSmBckfRVe48zAybqEn4x1ygX5kh2H7YdAsuRgqWTP6BDX72GG-I3CFoj8SsOYlXlXaSKAymuO2u8aLTVpTRJipfXq42r3GKWem-Z5Pc4kPynhRtttu-Hu2oQfswUg1d9pjwPai_6ONcHlquRh8u0tGQn947DnOVOUYG6MTkRf7bqfrychKqq7AaWC7JzURaMoTq6-oCEaw4d-GdeCEQ6Kr3rWrbUUzCRs1kX0dnxJcZ_dLZMRCVjq_snG7IzKtl24MD10UzkvLrlM6P606O94-T9SCORHsmZ4Hsj0o/file [following]\n",
            "--2020-04-01 18:01:20--  https://uc83f0f5708a2a1a8cdda08ea965.dl.dropboxusercontent.com/cd/0/inline2/A1BEyAMnFjkhKExR56wPlhsmyQQ0Kn6LehO50zWN3Kwj2UIvyUSmBckfRVe48zAybqEn4x1ygX5kh2H7YdAsuRgqWTP6BDX72GG-I3CFoj8SsOYlXlXaSKAymuO2u8aLTVpTRJipfXq42r3GKWem-Z5Pc4kPynhRtttu-Hu2oQfswUg1d9pjwPai_6ONcHlquRh8u0tGQn947DnOVOUYG6MTkRf7bqfrychKqq7AaWC7JzURaMoTq6-oCEaw4d-GdeCEQ6Kr3rWrbUUzCRs1kX0dnxJcZ_dLZMRCVjq_snG7IzKtl24MD10UzkvLrlM6P606O94-T9SCORHsmZ4Hsj0o/file\n",
            "Reusing existing connection to uc83f0f5708a2a1a8cdda08ea965.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53925062 (51M) [application/zip]\n",
            "Saving to: ‘hackathon_disease_extraction.zip.2’\n",
            "\n",
            "hackathon_disease_e 100%[===================>]  51.43M  31.2MB/s    in 1.6s    \n",
            "\n",
            "2020-04-01 18:01:23 (31.2 MB/s) - ‘hackathon_disease_extraction.zip.2’ saved [53925062/53925062]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igNvphif4JTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b55b1963-3e81-489c-a6c0-3e252c24c149"
      },
      "source": [
        "!unzip /content/hackathon_disease_extraction.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/hackathon_disease_extraction.zip\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               y\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0V2DnyW28wQ",
        "colab_type": "text"
      },
      "source": [
        "## 1. Importing libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiyvOiV623ev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "d2d54dfe-41bb-4331-e757-b52e8bf02360"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import unicodedata\n",
        " \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense\n",
        "from keras.layers import TimeDistributed, Dropout, Bidirectional\n",
        " \n",
        "# Defining Constants\n",
        " \n",
        "# Maximum length of text sentences\n",
        "MAXLEN = 180\n",
        "# Number of LSTM units\n",
        "LSTM_N = 150\n",
        "# batch size\n",
        "BS=48"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Va2krvq2-eQ",
        "colab_type": "text"
      },
      "source": [
        "## 2. Reading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ldUjkNP26ZA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "e78c1085-678e-40ee-f2a9-a6a5d748d3a6"
      },
      "source": [
        "# Reading the training set\n",
        "data = pd.read_csv(\"train.csv\", encoding=\"latin1\")\n",
        "data.head(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Doc_ID</th>\n",
              "      <th>Sent_ID</th>\n",
              "      <th>Word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Obesity</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>in</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Low-</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Middle-Income</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Countries</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>:</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Burden</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>,</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Drivers</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  Doc_ID  Sent_ID           Word tag\n",
              "0   1       1        1        Obesity   O\n",
              "1   2       1        1             in   O\n",
              "2   3       1        1           Low-   O\n",
              "3   4       1        1            and   O\n",
              "4   5       1        1  Middle-Income   O\n",
              "5   6       1        1      Countries   O\n",
              "6   7       1        1              :   O\n",
              "7   8       1        1         Burden   O\n",
              "8   9       1        1              ,   O\n",
              "9  10       1        1        Drivers   O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEDxukfI3B7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "51320081-70e2-4c61-b630-634de75ce0ed"
      },
      "source": [
        "# Reading the test set\n",
        "test_data = pd.read_csv(\"test.csv\", encoding=\"latin1\")\n",
        "test_data.head(10)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Doc_ID</th>\n",
              "      <th>Sent_ID</th>\n",
              "      <th>Word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4543834</td>\n",
              "      <td>30001</td>\n",
              "      <td>191283</td>\n",
              "      <td>CCCVA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4543835</td>\n",
              "      <td>30001</td>\n",
              "      <td>191283</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4543836</td>\n",
              "      <td>30001</td>\n",
              "      <td>191283</td>\n",
              "      <td>MANOVA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4543837</td>\n",
              "      <td>30001</td>\n",
              "      <td>191283</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4543838</td>\n",
              "      <td>30001</td>\n",
              "      <td>191283</td>\n",
              "      <td>my</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4543839</td>\n",
              "      <td>30001</td>\n",
              "      <td>191283</td>\n",
              "      <td>black</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4543840</td>\n",
              "      <td>30001</td>\n",
              "      <td>191283</td>\n",
              "      <td>hen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4543841</td>\n",
              "      <td>30001</td>\n",
              "      <td>191283</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4543842</td>\n",
              "      <td>30001</td>\n",
              "      <td>191284</td>\n",
              "      <td>Comments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4543843</td>\n",
              "      <td>30001</td>\n",
              "      <td>191284</td>\n",
              "      <td>on</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  Doc_ID  Sent_ID      Word\n",
              "0  4543834   30001   191283     CCCVA\n",
              "1  4543835   30001   191283         ,\n",
              "2  4543836   30001   191283    MANOVA\n",
              "3  4543837   30001   191283         ,\n",
              "4  4543838   30001   191283        my\n",
              "5  4543839   30001   191283     black\n",
              "6  4543840   30001   191283       hen\n",
              "7  4543841   30001   191283         .\n",
              "8  4543842   30001   191284  Comments\n",
              "9  4543843   30001   191284        on"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfaZOk--4r4W",
        "colab_type": "text"
      },
      "source": [
        "## 3. Creating Word & Tag dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBXw5Ci93EDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "8e2cef22-23d5-4c2f-9065-867cc3f5e1a2"
      },
      "source": [
        "print(\"Number of uniques docs, sentences and words in Training set:\\n\",data.nunique())\n",
        "print(\"\\nNumber of uniques docs, sentences and words in Test set:\\n\",test_data.nunique())\n",
        " \n",
        "# Creating a vocabulary\n",
        "words = list(set(data[\"Word\"].append(test_data[\"Word\"]).values))\n",
        "words.append(\"ENDPAD\")\n",
        " \n",
        "# Converting greek characters to ASCII characters eg. 'naïve café' to 'naive cafe'\n",
        "words = [unicodedata.normalize('NFKD', str(w)).encode('ascii','ignore') for w in words]\n",
        "n_words = len(words)\n",
        "print(\"\\nLength of vocabulary = \",n_words)\n",
        " \n",
        "tags = list(set(data[\"tag\"].values))\n",
        "n_tags = len(tags)\n",
        "print(\"\\nnumber of tags = \",n_tags)\n",
        " \n",
        "# Creating words to indices dictionary.\n",
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "# Creating tags to indices dictionary.\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of uniques docs, sentences and words in Training set:\n",
            " id         4543833\n",
            "Doc_ID       30000\n",
            "Sent_ID     191282\n",
            "Word        184505\n",
            "tag              3\n",
            "dtype: int64\n",
            "\n",
            "Number of uniques docs, sentences and words in Test set:\n",
            " id         2994463\n",
            "Doc_ID       20000\n",
            "Sent_ID     125840\n",
            "Word        139891\n",
            "dtype: int64\n",
            "\n",
            "Length of vocabulary =  257203\n",
            "\n",
            "number of tags =  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1m_7tQm4v3H",
        "colab_type": "text"
      },
      "source": [
        "## 4. Getting Train & Test Sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUqney_N4t7q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c1ef4b7f-4671-4ebb-943c-45085b908d50"
      },
      "source": [
        "def get_tagged_sentences(data):\n",
        "    '''\n",
        "    Objective: To get list of sentences along with labelled tags.\n",
        "    Returns a list of lists of (word,tag) tuples.\n",
        "    Each inner list contains a words of a sentence along with tags.\n",
        "    '''\n",
        "    agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(), s[\"tag\"].values.tolist())]\n",
        "    grouped = data.groupby(\"Sent_ID\").apply(agg_func)\n",
        "    sentences = [s for s in grouped]\n",
        "    return sentences\n",
        " \n",
        "def get_test_sentences(data):\n",
        "    '''\n",
        "    Objective: To get list of sentences.\n",
        "    Returns a list of lists of words.\n",
        "    Each inner list contains a words of a sentence.\n",
        "    '''\n",
        " \n",
        "    agg_func = lambda s: [w for w in s[\"Word\"].values.tolist()]\n",
        "    grouped = data.groupby(\"Sent_ID\").apply(agg_func)\n",
        "    sentences = [s for s in grouped]\n",
        "    return sentences\n",
        "# Getting training sentences in a list\n",
        "sentences = get_tagged_sentences(data)\n",
        "print(\"First 2 sentences in a word list format:\\n\",sentences[0:2])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 2 sentences in a word list format:\n",
            " [[('Obesity', 'O'), ('in', 'O'), ('Low-', 'O'), ('and', 'O'), ('Middle-Income', 'O'), ('Countries', 'O'), (':', 'O'), ('Burden', 'O'), (',', 'O'), ('Drivers', 'O'), (',', 'O'), ('and', 'O'), ('Emerging', 'O'), ('Challenges', 'O'), ('.', 'O')], [('We', 'O'), ('have', 'O'), ('reviewed', 'O'), ('the', 'O'), ('distinctive', 'O'), ('features', 'O'), ('of', 'O'), ('excess', 'O'), ('weight', 'O'), (',', 'O'), ('its', 'O'), ('causes', 'O'), (',', 'O'), ('and', 'O'), ('related', 'O'), ('prevention', 'O'), ('and', 'O'), ('management', 'O'), ('efforts', 'O'), (',', 'O'), ('as', 'O'), ('well', 'O'), ('as', 'O'), ('data', 'O'), ('gaps', 'O'), ('and', 'O'), ('recommendations', 'O'), ('for', 'O'), ('future', 'O'), ('research', 'O'), ('in', 'O'), ('low-', 'O'), ('and', 'O'), ('middle-income', 'O'), ('countries', 'O'), ('(', 'O'), ('LMICs', 'O'), (')', 'O'), ('.', 'O')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttWdVg9E4yRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4348b2e5-65b6-4445-e61f-894ad518bd4c"
      },
      "source": [
        "# Getting test sentences in a list\n",
        "test_sentences = get_test_sentences(test_data)\n",
        "print(\"First 2 sentences in a word list format:\\n\",test_sentences[0:2])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 2 sentences in a word list format:\n",
            " [['CCCVA', ',', 'MANOVA', ',', 'my', 'black', 'hen', '.'], ['Comments', 'on', 'repeated', 'measures', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7Lc0Ofa4-Y_",
        "colab_type": "text"
      },
      "source": [
        "## 5. Feature Extraction for DL Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mipi5jCV4-9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting words to indices for test sentences (Features)\n",
        "# Converting greek characters to ASCII characters in train set eg. 'naïve café' to 'naive cafe'\n",
        "X = [[word2idx[unicodedata.normalize('NFKD', str(w[0])).\n",
        "encode('ascii','ignore')] for w in s] for s in sentences]\n",
        " \n",
        "# Converting words to indices for test sentences (Features)\n",
        "# Converting greek characters to ASCII characters in test-set eg. 'naïve café' to 'naive cafe'\n",
        "X_test = [[word2idx[unicodedata.normalize('NFKD', str(w)).\n",
        "encode('ascii','ignore')] for w in s] for s in test_sentences]\n",
        " \n",
        "'''\n",
        "Padding train and test sentences to 180 words.\n",
        "Sentences of length greater than 180 words are truncated.\n",
        "Sentences of length less than 180 words are padded with a high value.\n",
        "'''\n",
        "X = pad_sequences(maxlen=MAXLEN, sequences=X, padding=\"post\", value=n_words - 1)\n",
        "X_test = pad_sequences(maxlen=MAXLEN, sequences=X_test, padding=\"post\", value=n_words - 1)\n",
        " \n",
        "# Converting tags to indices for test sentences (labels)\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "# Padding tag labels to 180 words.\n",
        "y = pad_sequences(maxlen=MAXLEN, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
        " \n",
        "# Making labels in one hot encoded form for DL model\n",
        "y = [to_categorical(i, num_classes=n_tags) for i in y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdErcd0v5D1Y",
        "colab_type": "text"
      },
      "source": [
        "## 6. Building Bidirectional LSTM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-tBDfmu5GYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4a9958da-7ccc-4155-d7b7-ccd9d3ea0968"
      },
      "source": [
        "# 180 dimensional word indices as input\n",
        "input = Input(shape=(MAXLEN,))\n",
        " \n",
        "# Embedding layer of same length output (180 dim embedding will be generated)\n",
        "model = Embedding(input_dim=n_words, output_dim=MAXLEN, input_length=MAXLEN)(input)\n",
        " \n",
        "# Adding dropout layer\n",
        "model = Dropout(0.2)(model)\n",
        " \n",
        "# Bidirectional LSTM to learn from both forward as well as backward context\n",
        "model = Bidirectional(LSTM(units=LSTM_N, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        " \n",
        "# Adding a TimeDistributedDense, to applying a Dense layer on each 180 timesteps\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model) # softmax output layer\n",
        "model = Model(input, out)\n",
        " \n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X, np.array(y), batch_size=BS, epochs=2, validation_split=0.05, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 181717 samples, validate on 9565 samples\n",
            "Epoch 1/2\n",
            " 25152/181717 [===>..........................] - ETA: 1:17:45 - loss: 0.0237 - acc: 0.9970"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ba3nW1x5M8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL4tk3F35Qw6",
        "colab_type": "text"
      },
      "source": [
        "## 7. Prediction on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gteEfbHA5PB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting on trained model\n",
        "pred = model.predict(X_test)\n",
        "print(\"Predicted Probabilities on Test Set:\\n\",pred.shape)\n",
        "# taking tag class with maximum probability\n",
        "pred_index = np.argmax(pred, axis=-1)\n",
        "print(\"Predicted tag indices: \\n\",pred_index.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHSKBzPB5S4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flatten both the features and predicted tags for submission\n",
        "ids,tagids = X_test.flatten().tolist(), pred_index.flatten().tolist()\n",
        " \n",
        "# converting each word indices back to words\n",
        "words_test = [words[ind].decode('utf-8') for ind in ids]\n",
        "# converting each predicted tag indices back to tags\n",
        "tags_test = [tags[ind] for ind in tagids]\n",
        "print(\"Length of words in Padded test set:\",len(words_test))\n",
        "print(\"Length of tags in Padded test set:\",len(tags_test))\n",
        "print(\"\\nCheck few of words and predicted tags:\\n\",words_test[:10],tags_test[:10])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLvge7qN5X-w",
        "colab_type": "text"
      },
      "source": [
        "## 8. Prepare Submission Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJKuYvE45USv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The task here is to convert padded fixed 180 dimensional predicted tags\n",
        "to variable length test set sentences.\n",
        "1. If the sentences have word length shorter than 180,\n",
        "   then predcited tags are skipped.\n",
        "2. If the sentences have word length longer than 180,\n",
        "   then all extra words are tagged with \"O\" tag class.\n",
        "'''\n",
        " \n",
        "i=0\n",
        "j=1\n",
        "predicted_tags = []\n",
        "counts = test_data.groupby('Sent_ID')['id'].count().tolist()\n",
        " \n",
        "for index,count in enumerate(counts):\n",
        "    if count <= MAXLEN:\n",
        "        predicted_tags.append(tags_test[i:i+count])\n",
        "    else:\n",
        "        predicted_tags.append(tags_test[i:i+MAXLEN])\n",
        "        out = ['O']*(count-MAXLEN)\n",
        "        predicted_tags.append(out)\n",
        " \n",
        "    i=j*MAXLEN\n",
        "    j=j+1\n",
        " \n",
        "predictions_final = [item for sublist in predicted_tags for item in sublist]\n",
        "print(\"\\nLength of test set words and predicted tags should match.\")\n",
        "print(\"Length of predicted tags:\",len(predictions_final))\n",
        "print(\"Length of words in test set:\",test_data['Word'].size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzmtl43P5bzL",
        "colab_type": "text"
      },
      "source": [
        "## 9. Writing the Submission File\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuC3EQK95aNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"sample_submission.csv\", encoding=\"latin1\")\n",
        "# Creating a dataframe in the submission format\n",
        "df_results = pd.DataFrame({'id':df['id'],'Sent_ID':df['Sent_ID'],'tag':predictions_final})\n",
        "# writing csv submission file\n",
        "df_results.to_csv('submission_final.csv',sep=\",\", index=None)\n",
        "df_results.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H5xNg-g5e3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}